\section{Overview}

\subsection{Machine Learning}
Machine learning is the field of study that focuses on training machines (e.g., computers) to identify patterns and derive logical conclusions \autocite{machDef}. It is technically an imitation of the human learning process and can be divided into different types based on the approach it uses to train the algorithm \autocite[5]{chapman}. The two main types are supervised and unsupervised learning \autocite[43]{dunham}: \\
\begin{itemize}
\item Supervised learning is used to classify instances to already known categories based on their characteristics. Algorithms that belong to this type are first trained on an already labeled with the possible categories dataset and, afterward, use their gained knowledge to classify new unlabeled datasets \autocite[24]{han}. Technically it is like a human trying to learn by first looking at all the correct answers and then, based on them, try to classify new ones \autocite[5]{chapman}. For example, a supervised learning problem would be images, each one containing one hand-written digit, and its label the number to which it corresponds. After training the algorithm with a labeled set, the algorithm would be capable of recognizing digits on images. \\
\item Unsupervised learning, on the other side, is used to group data without knowing the labels beforehand and without any training proceeding. This type of learning allows the model to learn by itself. Usually, a person with knowledge in the sector is needed to interpret and extract the knowledge from the created groups \autocite[25]{han}. In this case, a human would only know whether or not his answer is correct, but nothing about the way to the right answer \autocite[5]{chapman}. The above example using unsupervised learning would be giving as input the images to the algorithm without labeling the digits. The expected output would be ten clusters, each one corresponding to one of the digits from zero to nine. A person inspecting the results would be in the position to recognize which is the digit shown on the images of each cluster. \\
\end{itemize}
Apart from those two main categories, two additional categories are worth mentioning, semi-supervised learning, and reinforcement learning. \\
\begin{itemize}
\item Semi-supervised learning falls between the above two types. It consists of a small amount of labeled data and a large amount of unlabeled data. The labeled data serve as initial training, and the results are using the unlabeled data to improve the process and, on many occasions, spot outliers \autocite[25]{han}. Following \textcite{chapman}'s ratiocination, in real life, it would look like a person given a few correct answers as a starting point to solve more complex questions. Using the digits example, let us suppose that labeling all the pictures would be a very costly procedure, therefore someone would label a small amount of the images to use as the training set, and afterward apply the algorithm to the rest of the images to improve the model.
\item Reinforcement learning is a more interactive method, as opposed to the previous ones. It uses unlabeled data and requires rewards or penalties based on its behavior. This type of machine learning takes decisions that lead to the maximization of the reward taken (e.g., a numerical value) \autocite[2-3]{rein}. Similarly to unsupervised learning, it is like giving feedback to a person, but not by telling it whether the answer is correct or not, but by grading its performance. Then that person using its sought knowledge would try to improve itself \autocite[5]{chapman}. Therefore, the digits paradigm would be similar to the one of unsupervised learning. However, instead of trying to discover hidden patterns, it would try to find the output with the maximized reward (or minimized penalty), making different decisions based on its previous experience.
\end{itemize}

\begin{figure}[ht]
\includegraphics[width=\textwidth]{mach_learn_types}
\caption{Types of machine learning.}
\label{fig:types}
\end{figure}

\subsection{Clustering}
As indicated by the definitions given by \textcite{dunham} and \textcite{tanSteinKum}, clustering or cluster analysis is a type of unsupervised machine learning. It groups instances to create coherent sets based on their similarities and dissimilarities. The aim is that instances that belong to the same sets are as much alike and as much different from the instances of the other sets as possible \autocite{dunham, tanSteinKum}. \\
As \textcite{kantar} claims, humans are capable of detecting natural clusters from given data and compete with the clusters created by algorithms up until three-dimensional data. In real life, though, the dimensionality of the data examined is usually much higher than that, and therefore, if the cluster analysis were possible to be done by a human, it would not be as efficient \autocite[250]{kantar}. Since it is not humans that perform clustering, but algorithms, unexpected groups, and patterns can be discovered \autocite[444]{han}. \\
The usefulness of cluster analysis can be confirmed from its numerous applications in various fields of study. Some of them are mentioned by \textcite{clAnalysis}. Some of them briefly described are the following:
\begin{itemize}
\item Market research. A market researcher may want to use clustering to segment its audience and form its target groups. Furthermore, it can be useful to find the correlations between the financial measures of a company to its stock performance. \textcite{chakrapani} describes an example in the market of sports cars. Whether a person will buy a sports car is not affected by demographics, but by its lifestyle, clustering can help to spot those lifestyles that lead to such a purchase \autocite[9]{clAnalysis}.
\item Astronomy. Astronomers need to discover distinct groups out of large data sets. For example, on \textcite{astronomy}'s application, stars could be divided into groups differentiated by their volume and size based on their velocities \autocite[10]{clAnalysis}.
\item Psychiatry. Cluster analysis can group mental illnesses, detect patterns in people that commit suicide, investigate parasuicidal patients, and classify eating disorders. By further research on the results, new approaches can be proposed. \textcite{psychiatry} found three different types of people that attempt suicide. They differ mainly on the cruelty of the methods they chose and their motivation behind the attempt \autocite[10]{clAnalysis}.
\item Bioinformatics and genetics. As \textcite{clAnalysis} explain, the genetic code is complex, and its variations are responsible for the differences between organisms. In each cell, different genes are expressed, resulting in the production of several proteins. Combining different cell types makes an organism functional. In the case of alterations in the genetic code or malfunctions in those processes, diseases occur. Through clustering, it is possible to study the cause of a disease, investigate inheritance, discover the functionality of new gene sequences, and make research more efficient and affordable \autocite[12-13]{clAnalysis}. \textcite{bioinformatics} used clustering in their article to group genes that follow similar expression patterns in order to decrypt a gene's functionalities and regulative mechanisms \autocite{bioinformatics}.
\end{itemize}
Additional applications of clustering in the touristic sector will be analyzed more in-depth in section \ref{applications}.

\subsection{Types of Clustering}
Clustering algorithms can be categorized based on two criterions. The first one is the relationships between the produced clusters of one iteration with the ones of a previous iteration. The second one is the number of clusters an instance can be a part of. \\
Based on the first criterion, \textcite{tanSteinKum} describe the following types: \\
\begin{itemize}
\item Partitional clustering is a type of clustering which allows no overlapping between two or more sets of clusters, hence partitions the initial set to independent sets. This means that each instance can be part of exactly one cluster \autocite[492]{tanSteinKum}. Therefore, it could be described as:
\begin{eqnarray*}
\begin{array}{c}
C_{i} \neq \emptyset, \: i = \{1,..,K\} \\[4pt]
\bigcup_{i=1}^{K} C_{i}=X \\[4pt]
C_{i} \cap C{j} = \emptyset, \: i,j=\{1, ...,K\} \: and \: i \neq j
\end{array}
\end{eqnarray*}
where \(X\) is the initial set, \(K\) the number of partitions, and \(C_{i}\), \(C{j}\) the \(i^{th}\), \(j^{th}\) clusters \autocite[645]{survey}.
\item Hierarchical clustering produces clusters in each iteration, which are subsets of a cluster of the previous iteration. This can occur by starting from one cluster, which contains all instances, and repeatedly partition the available clusters to even smaller ones \autocite[492]{tanSteinKum}. Therefore, it could be described as:
\begin{eqnarray*}
\left.\begin{array}{c}
C_{i} \in H_{m}, C_{j} \in H_{l} \\[4pt]
m>l
\end{array}\right. \Rightarrow
\left\{\begin{array}{c}
C_{i} \subset C_{j} \\
or \\
C_{i} \cap C_{j} = \emptyset
\end{array}\right.
, \: i \neq j \: and \: m,l = 1, ...,Q
\end{eqnarray*}
\end{itemize}
where \(H_{m}, H_{l}\) partitions of the original set \(X\), \(Q\) is the number of partitions, and \(C_{i}, C_{j}\) subsets of any of the \(Q\) partitions \autocite[646]{survey}. \\
\textcite{tanSteinKum} mention three more types of clustering in their book, which are formed based on the second criterion and are the following: \\
\begin{itemize}
\item Exclusive clustering signifies that an instance can only be a part of only one cluster \autocite[492]{tanSteinKum}. Let us define the binary variables \(x_{i}\) for each instance \(x \in X \), where \(X\) are all the instances. If an instance \(x\) belongs to the \(i^{th}\) cluster \(x_{i}\) is set equal to 1, otherwise to 0. Then, if the number of clusters is \(n\), we can describe each cluster by the function:
\begin{eqnarray*}
C_{i} = \{(x, x_{i})|x \in X\}
\end{eqnarray*}
where \(C_{i}\) is the \(i^{th}\) cluster, and which has the following restrictions:
\begin{eqnarray*}
\sum_{i=0}^{n} x_{i} = 1, x_{i} \in \{0,1\}
\end{eqnarray*}
\item Overlapping or non-exclusive clustering signifies that an instance can be part of more than one cluster \autocite[492]{tanSteinKum}. We can describe each cluster the same way we did in exclusive clustering, just by adjusting the restrictions:
\begin{eqnarray*}
C_{i} = \{(x, x_{i})|x \in X\} \\
\sum_{i=0}^{n} x_{i} \geq 1, x_{i} \in \{0,1\}
\end{eqnarray*}
where \(C_{i}\) is the \(i^{th}\) cluster, \(x\) is a specific instance, \(x_{i}\) is the binary variable that describes \(x\) in the \(i^{th}\) cluster, \(X\) are all the instances and \(n\) is the number of clusters.
\item Fuzzy clustering signifies that every instance belongs to all the clusters \autocite[492]{tanSteinKum}. As \textcite{kantar} explains, the function that shows the membership of an element inside a cluster is called membership function (MF). The value of the MF that describes a given instance is a number between [0,1]. Each cluster can be described as follows:
\begin{eqnarray*}
C_{i} = \{(x, \mu_{C}[x])|x \in X\}
\end{eqnarray*}
where \(C_{i}\) is the \(i^{th}\) cluster, \(x\) is a specific instance, \(\mu\) is the MF function, and \( X\) is the set that contains all the instances \autocite[416]{kantar}.
\end{itemize}

\section{K-means}
K-means is a prototype-based, iterative algorithm in which instances are assigned to a cluster in each iteration. The basic algorithm requires as input K points, called centroids, where K is the number of the desired clusters. To define the cluster to which one instance belongs to, the algorithm calculates its distance from all the centroids and assigns it to the closest one. Finally, using the produced clusters calculates the new centroids and repeats until it reaches the desired set. Each cluster's mean needs to be defined and recalculated in each iteration to redefine the new centroids \autocite{dunham, tanSteinKum}. \\
\textcite{euclidean} explain that K-means is practically a minimization problem. The objective function (the function to be minimized) is the function that calculates the distances between one instance and its cluster's centroid. The smallest the sum of the distances of one cluster's centroid to its instances, the highest the homogeneity of the cluster \autocite[2]{euclidean}. \\
Disimalirites between data objects are their distance \autocite[69]{tanSteinKum}, though there are many different methods of calculating the distance between two instances, the most common one used is the Euclidean distance \autocite[648]{survey}. Using the Euclidean distance to calculate the distances between the centroid of a cluster, and the rest of the cluster's instances, for a one-dimensional dataset, the model is as follows:
\begin{eqnarray*}
d_{ij} = \sqrt{(c_{i}-x_{j})^2}
\end{eqnarray*}
where \(d_{ij}\) is the distance of the \(j^{th}\) element from the \(i^{th}\) cluster, \(c_{i}\) is the \(i^{th}\) centroid, and \(x_{j}\) is the \(j^{th}\) element \autocite[69]{tanSteinKum}. \\
In the simple case where there is only one numerical value describing each instance the cluster mean can use the basic mean definition from statistics as follows: \\
\begin{eqnarray*}
m_{i} = \frac{1}{m}\sum_{j=1}^{m}(x_{ij})
\end{eqnarray*}
where \(m_{i}\) is the mean of the \(i^{th}\) cluster, \(m\) is the number of instances that belong to the \(i^{th}\) cluster, and \(x_{ij}\) is the \(j^{th}\) instance of the \(i^{th}\) cluster \autocite[140]{dunham}. \\
As \textcite{dunham} describes, the termination techniques of the algorithm may vary. It could be that the clusters of each iteration are the same or almost the same or that the algorithm terminates after a fixed number of iterations \autocite[140]{dunham}.
\begin{algorithm}
\SetAlgoLined
\KwInput{\(D=\{x_{1},x_{2},...,x_{n}\} \)\newline
K initial centroids
}
\KwOutput{K cluster sets}
\While{termination condition is not reached}
{
Assign each element to the cluster of the closest centroid.\\
Recompute the new centroids.
}
\caption{K-means}\label{alg:kmeans}
\end{algorithm} \\
Algorithm \ref{alg:kmeans} shows the K-means' basic steps \autocite{dunham, tanSteinKum}. \\
The algorithm's time complexity is O(tkn), where t is the number of iterations, n the number of elements, and k the number of clusters. \autocite[141]{dunham}. It is considered to produce good results and handles better storage and noise compared to some hierarchical agglomerative algorithms \autocite[526]{tanSteinKum}. On the other side, it is not very scalable and time-efficient \autocite[141]{dunham}. Furthermore, outliers need to be handled exclusively in order for the clusters to be representative \autocite[506]{tanSteinKum}.Finally, since K-means bases on the euclidean measure, it works well with globular data, but not that well with other geometrical shapes nor with multi-dimensional (categorical) data \autocite[647, 649]{survey}.
\section{Applications in the touristic sector}
\label{applications}
As \textcite{data-drivenSegmentation} states, companies all around the globe use market segmentation to target their audiences more effectively and consume their resources more efficiently. Cluster analysis is a useful tool for the creation of those segments to create groups of people that seek similar benefits from their travel experiences. These groups not only help in the formation of marketing strategies but also to products that offer higher fulfillment to their consumers \autocite[17]{data-drivenSegmentation}. This type of segmentation was introduced by \textcite{Haley} in 1968 and is called benefit segmentation. The need for the creation of a new way of grouping target groups arose from the fact that tourists' behavior is mainly determined by the benefits they seek to satisfy and not by descriptive factors \autocite[31]{Haley}. \\
\subsection{Benefit segmentation of potential wellbeing tourists}
A sample application of benefit segmentation using cluster analysis describe \textcite{finland} in their article. Data about tourists were gathered in the region of Savonlinna, Finland, during the period with the highest footfall of the year through electronic questionnaires. The research aims to find the different segments of tourists based on the benefits they seek and then examine the interest of each segment on wellness holidays. \\
For the segment formulation, they used two algorithms, one hierarchical to find the best number of clusters, and K-means to create the clusters. The solution they proposed contained four distinct clusters. From the four clusters, two of them seemed to have a higher preference for wellness services, as designated by \textcite{finland} 'Culturals' and 'Sightseers'. Both clusters portrayed people that seem to favor attractions and also have a tendency to go back to the same destination for their vacation. Additionally, the 'Culturals' show a preference for cultural activities, where 'Sightseers' show a preference for sightseeing activities. The illustrated interpretation of the results is that the previous approach, which promoted wellbeing products based on nature, might not be the most appropriate. The suggested alternative claims that combining wellbeing with history, culture, diverse experiences, and attractions could be more efficient \autocite[308-312]{finland}. \\
\subsection{A Clustering Method for Categorical Data in Tourism Market Segmentation Research}
Another sample application that focuses more on the nature of the data to be analyzed describes \textcite{categorical}. Commonly, collected touristic data come from surveys, which, to a great extent, contain qualitative data. Furthermore, for the market segmentation to produce more representative results, many attributes should be used. Even so, most clustering methods used by marketers do not work well nor with multi-dimensional nor with categorical data \autocite[391]{categorical}. \\
\textcite{categorical} used a Bed and Breakfast (B\&B) survey in order to illustrate a more reliable way to deal with this type of data. First, they used multiple correspondence analysis (MCA) to produce some initial cluster groups, get a visualization of them, and understand them. Afterward, they performed cluster analysis, using the k-means algorithm, in order to find the market segments. They concluded in four clusters, from which the three of them seemed to be worthy of further analysis. The other cluster did not contain enough data to lead to meaningful interpretation. The first of the three segments contained respondents who were seeking a romantic experience and were less likely to return to the B\&B. Also, they were not very concerned about the cost and amenities provided. Tourists of the second cluster were looking for cozier, more peaceful, nature-related options and were more likely to go back to the same place if they liked it. They also preferred to take part in energetic activities and, similarly to the first cluster, do not care so much about price and amenities. The third cluster was technically a mixture of the two previous ones. The distinguishing factor was the eagerness of the people of the latter to socialize and the importance of price, value factors \autocite[394-395]{categorical}. \\
Taking into consideration the results of the k-means \textcite{categorical} used the geographical characteristics of each cluster and suggested the usage of different marketing approaches on each of the three regions that the respondents came from (Wisconsin, Minnesota, Illinois). The B\&B owners made some changes based on the formed clusters to improve their customer satisfaction. For example, they decided to offer a private breakfast to the first cluster, add more activities to please the second cluster, and provide information about social events to the people of the last cluster. Furthermore, they were planning on changing their marketing strategy based on the demographics provided by the research to better target their audience \autocite[395-396]{categorical}.